name: Scrape Parkruns

on:
  workflow_dispatch:  # Manual triggering

jobs:
  scrape_parkruns:
    runs-on: ubuntu-latest

    steps:
    # Step 1: Checkout the repository
    - name: Checkout Repository
      uses: actions/checkout@v3

    # Step 2: Set up Python
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'  # Use a supported version

    # Step 3: Install dependencies
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install requests beautifulsoup4 geopy

    # Step 4: Download and Parse parkruns.json
    - name: Download and Parse parkruns.json
      run: |
        python - <<EOF
import json

# Load the parkruns.json file
with open('parkruns.json', 'r') as f:
    data = json.load(f)

# Parse country URLs and event data
country_data = data['countries']
events = data['events']['features']

# Extract event coordinates and URLs (max 5 events for testing)
event_data = []
for event in events[:5]:  # Limit to first 5 events
    country_code = event['properties']['countrycode']
    short_name = event['properties']['eventname']
    coordinates = event['geometry']['coordinates']
    
    # Get country base URL
    country_url = country_data[str(country_code)]['url']
    if country_url:
        event_url = f"{country_url}/{short_name}/course"
        event_data.append({
            'name': short_name,
            'url': event_url,
            'coordinates': coordinates
        })

# Save the event data to a JSON file for the next step
with open('event_data.json', 'w') as f:
    json.dump(event_data, f, indent=2)
EOF

    # Subsequent steps...
